data='''<title>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</title>
    <abastract>
<p>Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named 'Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive discussion with the popular Chain-of-Thought (CoT) methods. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities.</p>
                        </abstract>

    <body>
<head n="1">Introduction</head><p>Large Language Models (LLMs), such as the GPT series <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020;</ref><ref type="bibr" target="#b19">OpenAI, 2023)</ref>, have witnessed a surge in popularity due to their profound impact on various real-world applications, including question answering <ref type="bibr" target="#b16">(Lu et al., 2023)</ref>, code generation <ref type="bibr" target="#b22">(Poesia et al., 2022)</ref>, and conversational agents <ref type="bibr">(Bozkurt, 2023)</ref>. The wide applicability and efficacy of these models have led to rapidly growing research on understanding and improving the use of LLMs. Meanwhile, it is widely acknowledged that the quality of the prompt generated by humans critically influences the response quality of the LLMs, emphasizing the importance of effective queries that prioritize specificity, detail, and precision <ref type="bibr" target="#b19">(OpenAI, 2022)</ref>. The sensitivity to prompt qualities, on the other hand, limits the usage of LLMs to their full potential and requires a systematic way to improve human's prompt.</p><p>Misunderstandings in interpersonal communications often arise when individuals, shaped by distinct subjective experiences, interpret the same message differently. In social science, such phenomena can be attributed to cognitive biases in frames in thought <ref type="bibr" target="#b6">(Druckman, 2001)</ref>. A single message, framed in different ways, can lead individuals to different conclusions. People habitually project their frames onto their received information, and only shift these frames when incongruence arises. In this work, we posit that LLMs also exhibit their own frames in thought, and it is not uncommon to observe a disparity between the frames used by humans and LLMs that caused the LLM's sensitivity to prompt qualities. Meanwhile, it can be challenging for humans to assess the clarity of their questions and to align their Figure <ref type="figure">1</ref>: Motivating examples of our study. We can observe several ambiguities in the question for LLMs. An LLM can interpret "even month" as the month with an even number of days, which diverges from human intention.</p><p>Motivating Example. <ref type="bibr" target="#b0">Allen-Zhu &amp; Li (2023)</ref> found that LLMs such as GPT-4 may not efficiently reason with their internal knowledge even if they can retrieve information accurately. As shown in Figure <ref type="figure">1</ref>, when posed with the query, "Was Mother Teresa born on an even month?" GPT-4 might mistakenly assert that August is an odd month. Based on this observation, <ref type="bibr" target="#b0">Allen-Zhu &amp; Li (2023)</ref> suggested that GPT-4 instead requires a Chain-of-Thought process-relying on user-led follow-up questions-to correct its previous wrong answers. However, we take a step further to investigate the intrinsic reason for LLM's inefficiency in answering such questions. As shown in the other responses in Figure <ref type="figure">1</ref>, when GPT-4 explains its reasoning, the model has several ambiguities toward the questions. For example, it may consider February as odd due to its irregular number of days and sometimes consider an even/odd month to be months with an even/odd number of days. Ambiguity in questions <ref type="bibr" target="#b5">(Cole et al., 2023;</ref><ref type="bibr">Kim et al., 2023a</ref>) is a recognized concern in benchmark datasets. For instance, it has been observed that the NLI datasets such as MultiNLI <ref type="bibr" target="#b43">(Williams et al., 2018)</ref> contain ambiguities, which are challenging even for human interpreters <ref type="bibr" target="#b15">(Liu et al., 2023)</ref>. Furthermore, our study uncovers that benchmark datasets commonly used for LLM evaluation <ref type="bibr" target="#b39">(Wei et al., 2022;</ref><ref type="bibr">bench authors, 2023)</ref> possess ambiguities that are imperceptible to humans but challenging for language models. These ambiguities cause LLMs to provide mistaken responses to unintended queries.</p>
<head>Original question</head><p>Self-rephrased question Was {person} born in an even day?</p><p>Was {person} born in an even month? Was {person} born in an even year?</p><p>Could you provide more information on whether the individual named {person} was born on a day that is an even number? This refers to dates such as the 2nd, 4th, 6th, 8th, and so on within a given month.</p><p>Can you provide the specific month of the year in which {person} was born to determine if it falls into an even-numbered month such as February, April, June, August, October, or December? What is the birth year of {person} and is it an even number?</p><p>Figure <ref type="figure">2</ref>: Comparison with the original question <ref type="bibr" target="#b0">(Allen-Zhu &amp; Li, 2023)</ref> and one possible self-rephrased question of GPT-4. We can observe a marked improvement of accuracy in answering these rephrased questions over the query of 105 celebrities for this task.</p><p>Our Approach. In this paper, we highlight an often-overlooked aspect of studies in LLMs: the disparity between human and LLM thought frames. Our research illustrates that this disparity significantly impacts the performance of LLMs in the zero-shot setting. To tackle this problem, we propose to let the LLM rephrase the question and incorporate additional details for better answering. We observe that, as opposed to questions asked casually by human, the automatically rephrased questions tend to enhance semantic clarity and aid in resolving inherent ambiguity. This self-rephrasing technique leads to significant improvement in accuracy, as shown in the barplot of Figure <ref type="figure">2</ref>.</p><p>Building upon these insights, we introduce a method named Rephrase and Respond (RaR), which prompts the LLM to rearticulate the given question and respond in a single prompt. Our empirical results across diverse reasoning tasks show the effectiveness of the approach. Additionally, we leverage RaR to facilitate the transfer of rephrased questions from more capable LLMs to clarify ambiguities for the less advanced models. Lastly, we present thorough discussions with the Chain-of-Thought (CoT) method <ref type="bibr" target="#b13">(Kojima et al., 2022;</ref><ref type="bibr" target="#b39">Wei et al., 2022)</ref>. RaR is complementary to CoT and can be easily combined for improvement, as confirmed by our experimental results. Unlike methods that employ multiple LLMs for iterative prompt engineering based on accuracy scores <ref type="bibr">(Zhou et al., 2022b;</ref><ref type="bibr" target="#b25">Pryzant et al., 2023)</ref>, our method is both unsupervised and training-free, making it economical and applicable to all questions in the zero-shot setting. Lastly, our work calls forth the importance that the design of human-crafted tasks targeting specific LLM capabilities should be rigorously reviewed by both humans and LLMs to ensure clarity in intention.</p>
<head n="2">Related Work</head>
<head n="2.1">Prompting Methods for LLMs</head><p>Since the advent of recent LLMs <ref type="bibr" target="#b19">(OpenAI, 2023;</ref><ref type="bibr" target="#b35">Touvron et al., 2023;</ref><ref type="bibr" target="#b4">Chiang et al., 2023)</ref>, a growing body of research has focused on prompt engineering for LLMs <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b28">Schick &amp; Sch ütze, 2021;</ref><ref type="bibr">Zhou et al., 2022b;</ref><ref type="bibr" target="#b42">White et al., 2023;</ref><ref type="bibr" target="#b37">Wang et al., 2023)</ref>. Manual guidelines have emerged to guide users in designing and revising their prompts <ref type="bibr">(Reynolds &amp; McDonell, 2021;</ref><ref type="bibr">Saravia, 2022)</ref>. OpenAI (2022) has also offered general recommendations for crafting queries, emphasizing specificity, detail, and precision. However, individuals often find it challenging to refine their own questions for clarity or to include necessary details for LLMs, as the questions are clear enough for humans themselves. Subsequent research <ref type="bibr">(Zhou et al., 2022b;</ref><ref type="bibr" target="#b32">Sorensen et al., 2022;</ref><ref type="bibr" target="#b25">Pryzant et al., 2023)</ref> has concentrated on the autonomous refinement of prompts. These methods often employ multiple LLMs to generate candidate prompts, evaluate and score these prompts, and iteratively refine them until a satisfactory prompt is produced. The evaluation of a prompt typically relies on either the accuracy of an LLM's response <ref type="bibr">(Zhou et al., 2022b;</ref><ref type="bibr" target="#b25">Pryzant et al., 2023)</ref> or the mutual information of the question <ref type="bibr" target="#b32">(Sorensen et al., 2022)</ref>. Given the nature of iterative computation and the necessity for qualitative evaluation, such methods are employed for refining single prompt templates; applying them universally to all questions would be expensive.</p><p>The method most frequently used by users and closely aligned with our approach is the zero-shot Chain-of-Thought (CoT) prompting <ref type="bibr" target="#b13">(Kojima et al., 2022)</ref>. In the stronger fewshot setting, few-shot CoT <ref type="bibr" target="#b39">(Wei et al., 2022)</ref> has also been a strong prompting method. Given that these techniques do not require evaluation and iterative selection, they have gained widespread popularity and inspired a series of subsequent studies <ref type="bibr" target="#b38">(Wang et al., 2022;</ref><ref type="bibr">Zhou et al., 2022a;</ref><ref type="bibr" target="#b24">Press et al., 2022;</ref><ref type="bibr" target="#b46">Yao et al., 2023;</ref><ref type="bibr">Zhang et al., 2023b;</ref><ref type="bibr" target="#b30">Shao et al., 2023)</ref>. However, CoT methods are not without their limitations, as observed in our study. Recent investigations have also highlighted challenges with the reliability of both zero-shot CoT <ref type="bibr" target="#b36">(Turpin et al., 2023)</ref> and few-shot CoT <ref type="bibr" target="#b21">(Pawelczyk et al., 2023)</ref>. Most recently, <ref type="bibr" target="#b52">Zhou et al. (2023)</ref> propose Foresee and Reflect similarly as a zero-shot prompting method that targets the proposed task Thinking for Doing (T4D). Lastly, it is worth noting that our method is complementary to all the prompting techniques mentioned above and can be combined.</p>
<head n="2.2">Self-correction Methods for LLMs</head><p>Another line of work aims at enhancing LLM performance <ref type="bibr" target="#b17">(Madaan et al., 2023;</ref><ref type="bibr" target="#b40">Welleck et al., 2022;</ref><ref type="bibr">Kim et al., 2023b;</ref><ref type="bibr" target="#b16">Pan et al., 2023;</ref><ref type="bibr" target="#b31">Shinn et al., 2023)</ref> by leveraging the LLM to refine its own responses, a concept known as post-hoc prompting. This encompasses terms such as "self-correction", "self-refine", and "self-critique", where LLMs revise their own responses drawing upon various feedback sources or critic models. As classified by <ref type="bibr" target="#b16">Pan et al. (2023)</ref>, automated critic models generally employ the LLM's self-feedback <ref type="bibr">(Madaan et al., Preprint 2023;</ref><ref type="bibr" target="#b31">Shinn et al., 2023;</ref><ref type="bibr" target="#b44">Yan et al., 2023)</ref>, other trained LLMs <ref type="bibr" target="#b45">(Yang et al., 2022;</ref><ref type="bibr" target="#b14">Lightman et al., 2023)</ref>, or external references <ref type="bibr" target="#b10">(Jung et al., 2022;</ref><ref type="bibr" target="#b8">Gao et al., 2023;</ref><ref type="bibr">Yu et al., 2023b;</ref><ref type="bibr" target="#b40">Welleck et al., 2022)</ref>. Yet, recent studies <ref type="bibr" target="#b9">(Huang et al., 2023;</ref><ref type="bibr" target="#b33">Stechly et al., 2023)</ref> examine the self-correction capacities of LLMs and find potential limitations, suggesting that LLMs may not be able to self-correct their reasoning processes. Their findings reveal that self-correction is no better than self-consistency <ref type="bibr" target="#b38">(Wang et al., 2022)</ref>. Contrary to allowing the LLM to self-refine its responses, our method let the LLM instead rephrase questions originally crafted by humans.</p>
<head>Concurrent works.</head><p>To the best of our knowledge, our work is one of the first works to examine the misunderstandings between humans and LLMs, as well as the models' ability to rephrase inputs as a means of self-improvement. Concurrently, several studies have emerged that also explore and utilize the rephrasing capabilities of LLMs. Instead of zeroshot prompting, <ref type="bibr">Yu et al. (2023a)</ref> employs LLMs to rephrase questions for the generation of synthetic data and subsequent fine-tuning. In the domain of vision-language models, <ref type="bibr" target="#b23">Prasad et al. (2023)</ref> similarly shows that the rephrasing ability of language models can effectively improve its own reasoning process. Furthermore, <ref type="bibr" target="#b41">Weston &amp; Sukhbaatar (2023)</ref> introduces System 2 Attention, a method that let the LLM itself filter out irrelevant information from the original prompt and to regenerate the query.</p>
<head n="3">Aligning the Frames with Rephrase and Respond</head><p>To systematically reduce prompt ambiguity and contextualize information in a way that aligns with the existing frame of the LLMs, we propose Rephrase and Respond (RaR) as a means of self-clarification. In interpersonal communication, rephrasing is a commonly known technique where people rephrase another person's question as a process of understanding, to ensure clarity and coherence in responding. Such a communication strategy can therefore be similarly applied to an LLM, letting it generate a rephrased question first and provide an answer subsequently. Following this intuition, RaR asks the LLMs to Rephrase the given questions and then Respond within a single query. This approach can be viewed as a strategy to directly enhance the quality of the LLM's response. In detail, we introduce the following prompt for the question-answering task:</p><p>"{question}"\nRephrase and expand the question, and respond.</p><p>(3.1)</p><p>Our prompt design aimed for conciseness and effectiveness. Meanwhile, many other variations of the prompt remain within our methodology and also provide improvement in performance, which share the same intention of rephrasing before responding. Such prompts include but not limited to the following,</p><p>• Reword and elaborate on the inquiry, then provide an answer.</p><p>• Reframe the question with additional context and detail, then provide an answer. • Modify the original question for clarity and detail, then offer an answer. • Restate and elaborate on the inquiry before proceeding with a response.</p><p>In later experiments, we will show that RaR remains effective for all variations of these prompts. With rephrasing before answering, RaR offers several desirable properties including (1) letting the LLM self-improve the prompts while maintaining the context of the original query, (2) better aligning the human's intended query with LLM's preferred style of question, (3) expanding the LLM's thought process and adding a step that will not naturally appear when using CoT and (4) providing an approach for humans to interpret how LLMs understand the questions.</p>
<head n="4">RaR Effectively Improves LLM Responses</head><p>In this section, we provide a comprehensive assessment of the applicability and efficacy of RaR. The results are presented in four primary dimensions: (1) RaR is a simple and effective prompt to improve LLM performances;</p><p>(2) variations of prompts still fall in the methodology of RaR and remain robustly effective;</p><p>(3) LLMs, while all benefit from RaR, have different proficiency in rephrasing and benefit at different levels. The last letters in the words "Edgar" and "Bob" are "r" and "b", hence when concatenated, it forms "rb". </p>
<head n="4.1">Benchmark Tasks</head><p>We evaluate the zero-shot capabilities of LLMs across multiple benchmark tasks in different categories. As proposed by <ref type="bibr" target="#b0">Allen-Zhu &amp; Li (2023)</ref>, we consider the following tasks that even advanced LLMs such as GPT-4 are found to respond poorly to many of these questions<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>• Knowledge Classification <ref type="bibr" target="#b0">(Allen-Zhu &amp; Li, 2023)</ref>. Sampling a pool of individuals with Wikipedia pages, this task challenges the LLM to decide if a renowned person was born on an even day, month, or year. • Knowledge Comparison <ref type="bibr" target="#b0">(Allen-Zhu &amp; Li, 2023)</ref>. Using the same pool of individuals, this task instructs the LLM to compare the ages of two people and decide who was born earlier.</p><p>Furthermore, we consider the following widely-used datasets for a comprehensive evaluation, which are also widely considered in previous prompting works <ref type="bibr" target="#b39">(Wei et al., 2022)</ref>.</p><p>• CSQA <ref type="bibr" target="#b34">(Talmor et al., 2019)</ref>. The CommonSense QA data encompasses a range of questions that evaluate the ability of commonsense understanding of the world and involves intricate semantics. • Date Understanding <ref type="bibr">(bench authors, 2023)</ref>. Sourced from Big-bench (bench authors, 2023), the Date Understanding task emphasizes commonsense reasoning and deducing a date from a provided context. We consider a more difficult version where we do not provide the choices of potential answers and let the LLM answer directly. • Last Letter Concatenation <ref type="bibr" target="#b7">(Fortes, 2023)</ref>. The task centers on symbolic reasoning, and asks the LLM to concatenate the final letters of a given list of names. We consider concatenation for two names as well as a more difficult task of concatenation for four names. • Coin Flip<ref type="foot" target="#foot_1">foot_1</ref> . Sourced from Hugging Face, the task asks the LLM if the coin still heads up, given its initial condition and subsequent actions of people who either flipped or did not flip the coin. We add an additional "Flip means reverse." to the questions. • Sports (bench authors, 2023). Sourced from Big-bench (bench authors, 2023), the Sports Understanding task primarily asks if a sentence is plausible or implausible, where a prominent sports figure is depicted performing specific sports-related actions.</p><p>The details of all evaluated tasks are summarized in Table <ref type="table">5</ref> of Appendix A.</p>
<head n="4.2">Main Results</head><p>We conduct experiments on the aforementioned benchmark datasets using GPT-4<ref type="foot" target="#foot_2">foot_2</ref> (OpenAI, 2023). We investigate the performance of RaR, which allows the LLM to both rephrase and respond to the question in a single query. Such an approach can be considered as a simple zero-shot strategy to improve the LLM's performance on any question. In Figure <ref type="figure" target="#fig_2">4</ref> Preprint and detailed in Table <ref type="table">1</ref>, we compare the accuracy of GPT-4 with or without RaR. Across a diverse span of 10 tasks that emphasize different aspects of LLM's capabilities, RaR consistently yields distinguishable improvements for GPT-4. Notably, for tasks that GPT-4 originally finds highly challenging (e.g., last letter concatenation), RaR exhibits remarkable improvement even to almost 100% accuracy. Indeed, similar to human communication, rephrasing and elaborating a question and then answering is an effective approach. In summary of the results, RaR provides a universal, plug-and-play zero-shot prompt that allows for efficient and effective performance improvement of LLMs on general tasks.  For tasks such as CSQA where the question quality is good enough, our method still offers a marginal improvement.</p><p>Variations of Prompt. Our primary findings, as illustrated in Figure <ref type="figure" target="#fig_2">4</ref>, were obtained using the specific prompt (3.1). However, it is important to note that variations of this prompt with modified phrases remain aligned with the RaR methodology's underlying principles. In Table <ref type="table">1</ref>, we show the performance of GPT-4 across the four variations of the RaR prompt, as listed in Section 3, along with the average performance deviations from the results of our main prompt. Despite the fact that changes in prompt wording induce minor fluctuations in GPT-4's zero-shot performance, a consistent enhancement relative to the baseline results is observed across all variations.</p><p>Table <ref type="table">1</ref>: Zero-shot test accuracy of GPT-4 on the original questions of each task, using different variations of prompts. We denote the difference in average zero-shot accuracy for prompt variants with the main prompt. </p>
<head n="4.3">Performance across Various LLMs</head><p>We further examine the performance of RaR on various LLMs, including GPT-3.5 and Vicuna <ref type="bibr" target="#b4">(Chiang et al., 2023)</ref>, a fine-tuned model from LLaMA-2, to investigate if different LLM architectures can provide consistent response improvement by rephrasing the questions. Specifically, we consider GPT-4-0613, GPT-3.5-turbo-0613, and Vicuna-13b-v1.5. We present the experiment results in Figure <ref type="figure">5</ref>. Due to Vicuna-13b-v1.5's near-zero performance on Last Letter Concatenation (4), we exclude this task from the evaluation of Vicuna-13b-v1.5. Remarkably, all examined LLMs demonstrate enhanced performance with RaR, resulting in a notable increase in accuracy across the majority of the tasks. More advanced models, such as GPT-4, benefit from the most significant gains across all tasks, while models of lesser complexity, like Vicuna, achieve modest improvements using our approach. On certain tasks such as Sports, GPT-3.5 and Vicuna even exhibit slightly diminished performance. In Appendix B.1, we provide specific examples of the rephrased questions across different models to illustrate the performance differences. We wrap up this experiment with the following key insight: all models can benefit from rephrasing questions, with more advanced models expected to gain a larger improvement.</p><p>GPT-4-0613 GPT-3.5-turbo-0613 Vicuna-13b-v1.5</p><p>Figure <ref type="figure">5</ref>: Accuracy (%) of GPT-4-0613, GPT-3.5-turbo-0613 and Vicuna-13b when testing on original and self-rephrased questions using Two-step RaR. GPT-4 enjoys larger improvement on various tasks compared with GPT -3.5 and Vicuna.</p>
<head n="5">Let Stronger LLMs Rephrase for Weaker LLMs to Respond</head><p>While different LLMs possess different rephrasing capabilities and benefit at different levels from RaR, we further pose the question that: can stronger LLMs be leveraged to help weaker LLMs in clarification of the prompts? To further utilize the quality improvement of the questions rephrased by larger models, like GPT-4, we introduce a variation of RaR called Two-step RaR that seprates the rephasing step from responding step in RaR. Intuitively, even among humans, a more detailed and precise question elicits more accurate and decisive responses. Two-step RaR follows this intuition by designing a two-step procedure to improve the quality of the questions: in the first step, given a query question, we generate a self-rephrased query rephrased question by prompting a rephrasing LLM with the following prompt:</p><p>"{question}" Given the above question, rephrase and expand it to help you do better answering. Maintain all information in the original question.</p><p>Then the original question and the rephrased question are combined to prompt a responding LLM with the following prompt:</p><p>(original) {question} (rephrased) {rephrased question} Use your answer for the rephrased question to answer the original question.</p><p>Notably, the rephrasing LLM and the responding LLM can be either the same or different models. As we shown in experiments, different LLMs exhibit distinct proficiency in question rephrasing. Here, we examine if the rephrased questions generated by a stronger LLM (GPT-4) can benefit a weaker LLM (Vicuna). We detail Vicuna-13b-v1.5's performance on questions rephrased by GPT-4, as compared to its own rephrased questions in Table <ref type="table" target="#tab_2">2</ref>. Consistent with our expectation that GPT-4 can better align with human intention and clarify the question, we observe that its rephrased questions remarkably enhance Vicuna-13b-v1.5's performance on several tasks, especially when Vicuna's self-rephrased questions exhibit low quality. Indeed, the questions can be clarified further for Vicuna, but more exploration needs to be made on its capability of self-rephrased questions.</p><p>Preprint Tasks. We examine the Chinese Idiom task as introduced in Allen-Zhu &amp; Li (2023), specifically the most difficult task of inferring the first letter. This task involves taking widely recognized four-character Chinese idioms and masking one character at one position to let LLM correctly infer the masked character. Furthermore, we also use the StereoSet task <ref type="bibr" target="#b18">(Nadeem et al., 2021)</ref>, which assesses the stereotypical biases present in LLMs with respect to gender, race, profession, and religion. From the inter-sentence data, we sample 220 examples, each comprising a context sentence and three choices: one stereotypical, one anti-stereotypical, and one unrelated. We adopt the prompt format used by <ref type="bibr" target="#b29">Shaikh et al. (2022)</ref>. We defer the detailed explanation of evaluation to Appendix A.</p>
<head>Results.</head><p>As illustrated in Table <ref type="table" target="#tab_3">3</ref>, even though RaR enhances LLM's performance, accurately inferring the first character of the Chinese Idiom task remains a challenge. One might then ask: does zero-shot CoT provide consistent improvement to LLM in such tasks as it does on other reasoning tasks? Our discovery is, in fact, zero-shot CoT may result in worse performances (31.43%) for such hard tasks, as the LLM tends to hallucinate during the intermediate steps-a phenomenon similar to hallucination snowballing <ref type="bibr">(Zhang et al., 2023a)</ref>. Furthermore, as <ref type="bibr" target="#b29">Shaikh et al. (2022)</ref> discovered on other language models, zeroshot CoT may result in undesired reasoning towards bias and toxicity. Also in Table <ref type="table" target="#tab_3">3</ref>, we demonstrate the performance of GPT-4 on StereoSet. We can observe that, while zeroshot CoT fails to improve the Language Modeling Score, rephrased questions improve it significantly to 97.73%. This implies that, with RaR, the LLM rarely opts for unrelated choices. Moreover, while zero-shot CoT improves the percentage of fair responses (choosing neither of them), RaR achieves the best performance. Tasks. We revisit the Last Letter Concatenation task and refer to the few-shot examples provided in <ref type="bibr" target="#b39">Wei et al. (2022)</ref>. As shown in Figure <ref type="figure" target="#fig_3">6</ref>, the examples follow a specific logic: obtain the last letter of the first and second word, and concatenate these letters, resulting in the answer. Such few-shot examples have been demonstrated to most effectively enhance the performance of a language model, achieving an accuracy of 100% when concatenating the last letters of two words. Conversely, we explore an example that employs the following logic: obtain the first letter of the first and second word, and concatenate these letters, providing the answer for last letter concatenation. Our aim is to investigate how this alternative few-shot prompt, despite bearing a logic similar to the original prompt and the correct answer, influences the performance of the GPT-4.</p><p>Results. As illustrated in Figure <ref type="figure" target="#fig_3">6</ref>, GPT-4 tends to stick to the logic of our modified prompt, resulting in an incorrect answer. It accurately concatenates all first letters, but concludes with a seemingly arbitrary final answer. In Table <ref type="table" target="#tab_4">4</ref>, we demonstrate the results of oneshot and four-shot CoT using such examples. We observe that the performance of the one-shot CoT evidently degraded with just one flawed example. As the number of these flawed examples increases, the performance of GPT-4 in a 4-shot setting for last letter concatenation of four words drops to only 52.27%. This observation reveals a potential pitfall in employing few-shot CoT: given that these examples are user-crafted, their quality becomes vital. Meanwhile, we discovered that RaR enables GPT-4 to correct any pitfalls in the logic of the given examples.</p><p>GPT-4 Q: Take the last letters of the words in "Elon Musk" and concatenate them. A: The first letter of "Elon" is "E". The first letter of "Musk" is "M". Conca--tenating them is "EM". The answer is nk. Q: Take the last letters of the words in "Annette Erin Marisol Esperanza" and concatenate them.</p><p>The first letter of "Annette" is "A". The first letter of "Erin" is "E". The first letter of "Marisol" is "M". The first letter of "Esperanza" is "E". Concatena--ting them is "AEME". The answer is etaz.</p><p>GPT-4</p><p>Q: Take the last letters of the words in "Elon Musk" and concatenate them. A: The last letter of "Elon" is "n". The last letter of "Musk" is "k". Conca--tenating them is "nk". The answer is nk. Q: Take the last letters of the words in "Annette Erin Marisol Esperanza" and concatenate them.</p><p>The last letter of "Annette" is "e". The last letter of "Erin" is "n". The last letter of "Marisol" is "l". The last letter of "Esperanza" is "a". Concatena--ting them is "enla". The answer is enla.</p><p>"Q: Take the last letters of the words in "Elon Mu--sk" and concatenate them. A: The first letter of "Elon" is "E". The first letter of "Musk" is "M". Concatenating them is "EM". The answer is nk."</p><p>Given the above question, rephrase and expand it to help you do better answering. Maintain all inf--ormation in the original question.</p><p>"Q: Identify the last letters of each word in the n--ame "Elon Musk", then put those letters together. A: The last letter of 'Elon' is 'n'. The last letter of 'Musk' is 'k'. When you put these letters together, the result is 'nk'."  </p>
<head n="7">Conclusion</head><p>In this paper, we have investigated the existing misunderstandings that occur between humans and LLMs and demonstrated that questions that appear clear to humans may still be misinterpreted by LLMs. Building on this insight, we introduced Rephrase and Respond Preprint (RaR), a novel approach that prompts an LLM to first rephrase and clarify the question before answering it. Our empirical evaluations, conducted across a range of benchmark datasets, confirm the effectiveness of our proposed methods. Further analysis reveals that while all models gain enhanced performance through question rephrasing, the more sophisticated models exhibit more substantial improvements. Based on the observation, We presented Two-step RaR, a variation of RaR that employs a rephrasing LLM to refine questions for subsequent use by any responding LLM. Crucially, we have found that the enhancement in question quality achieved through rephrasing is transferable across models.</p><p>In addition to these findings, we have made comparisons with CoT methods through both mathematical formulation and empirical investigations. We also demonstrated that RaR is complementary to CoT, and can be leveraged to achieve additional performance gains. Q: Take the last letters of the words in "Elon Musk" and concatenate them.</p><p>A: The first letter of "Elon" is "E". The first letter of "Musk" is "M". Concatenating them is "EM". The answer is nk. Q: Take the last letters of the words in "Larry Page" and concatenate them.</p><p>A: The first letter of "Larry" is "L". The first letter of "Page" is "P". Concatenating them is "LP". The answer is ye. Q: Take the last letters of the words in "Sergey Brin" and concatenate them.</p><p>A: The first letter of "Sergey" is "S". The first letter of "Brin" is "B". Concatenating them is "SB". The answer is yn. Q: Take the last letters of the words in "Bill Gates" and concatenate them.</p><p>A: The first letter of "Bill" is "B". The first letter of "Gates" is "G". Concatenating them is "BG". The answer is ls. Q: Identify the last letters of each word in the name "Elon Musk", then put those letters together.</p><p>A: The last letter of 'Elon' is 'n'. The last letter of 'Musk' is 'k'. When you put these letters together, the result is 'nk' Q: Identify the last letters of each word in the name "Larry Page", then put those letters together.</p><p>A: The last letter of 'Larry' is 'y'. The last letter of 'Page' is 'e'. When you put these letters together, the result is 'ye' Q: Identify the last letters of each word in the name "Sergey Brin", then put those letters together.</p><p>A: The last letter of 'Sergey' is 'y'. The last letter of 'Brin' is 'n'. When you put these letters together, the result is 'yn' Q: Identify the last letters of each word in the name "Bill Gates", then put those letters together.</p><p>A: The last letter of 'Bill' is 'l'. The last letter of 'Gates' is 's'. When you put these letters together, the result is 'ls'  Initial observations suggest that Vicuna-13b-v1.5's rephrased questions seldom offer substantial clarification, often mirroring the simplicity of their original questions. In the last instance of Table <ref type="table">9</ref>, Vicuna-13b-v1.5 perturbs the question's intent by changing "yesterday" Table <ref type="table">10</ref>: Examples of the self-rephrased questions generated by GPT-4 after the 1st, 2nd and 3rd self-rephrasing. The original question is "Was Abraham Lincoln born in an even day?" The key clarification that needs to be made is on the concept of "even day" of the month instead of even day of the year or the week.  Question: Was Stephen Hawking born in an even day? GPT-4 (Incorrect): No, Stephen Hawking was born on January 8, 1942. Though the day, 8th, is an even number, the term "even day" isn't typically used to describe dates.</p>
<head>B Additional Experiment Results</head>
<head>99</head><p>Question: Was Frida Kahlo born in an even day? GPT-4 (Incorrect): No, Frida Kahlo was born on July 6, 1907. Therefore, she was born on an odd day.</p><p>Question: Was Leonardo da Vinci born in an even day? GPT-4 (Correct): No, Leonardo da Vinci was born on April 15, 1452, which is an odd day.</p><p>Question: Was Ludwig van Beethoven born in an even day? GPT-4 (Correct): No, Ludwig van Beethoven was born on December 17, 1770, which is an odd day.</p><p>Table <ref type="table" target="#tab_2">12</ref>: Examples of correct and incorrect responses by GPT-4 to 1-step RaR on Even day.</p><p>We omit "\n" and leave it as a blank space for better readability. Question: "Was Barack Obama born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Correct): "Did the former United States President, Barack Obama, have his birthday fall on an even numbered day of a month?" Barack Obama was born on August 4, 1961. So yes, he was born on an even numbered day.</p><p>Question: "Was Kanye West born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Correct): The question asks, "Was the day of Kanye West's birth an evennumbered day?" Kanye was born on June 8, 1977. Thus, yes, he was born on an even day, the 8th of June.</p><p>Question: "Was Stephen Hawking born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Correct): The expanded version of the question could be phrased as, "Was renowned physicist and author Stephen Hawking born on a date that is classified as an even day?". Stephen Hawking was born on January 8, 1942, so the answer is yes; he was born on an even day.</p><p>Question: "Was Frida Kahlo born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Correct): "Did the famous Mexican artist Frida Kahlo have her birth date fall on an even numbered day of the month?" Yes, she was. Frida Kahlo was born on July 6th, 1907, which is an even day of the month.</p><p>Question: "Was Leonardo da Vinci born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Incorrect): "Did the day of birth for the renowned artist and inventor, Leonardo da Vinci, fall on an even-numbered day?" Yes, Leonardo da Vinci was born on April 15, 1452, which is an odd-numbered day.</p><p>Question: "Was Ludwig van Beethoven born in an even day?" Rephrase and expand the question, and respond. GPT-4 (Correct): "Was the celebrated composer Ludwig van Beethoven born on a day that falls under an even-numbered date?" Ludwig van Beethoven was born on December 17, 1770. Since 17 is an odd number, the answer is no, Beethoven was not born on an even day. Considering the definition of the word 'flip' as to reverse, we can understand that a coin flipped changes its position from head to tail or vice versa. So, would the coin maintain its initial position of being heads up after gitta flipped it?Answer: No. The initial state of a coin is that it is face up with heads. Then, Aluino performs an action of flipping the coin, which implies reversing its current face from heads to tails. Following this, Arthor takes his turn and also flips the coin. Now, can it be determined whether the coin reverts to its original state of being heads up after these sequential flips? Remember, in this context, flipping pertains to reversing the side that the coin is currently showing. Answer the Yes or No question.</p><p>GPT-4 (Correct): Yes.</p><p>Question: (original) A coin is heads up. jumaimah flips the coin. buity flips the coin. Is the coin still heads up? Flip means reverse. (rephrased) The initial position of the coin is heads up. Jumaimah proceeds to perform an action on the coin known as a flip, which changes the coin from its current position to its opposite position. Following this, Buity also conducts the same action on the coin as Jumaimah did, which is a flip. With this sequence of events, can we determine if the final position of the coin is still heads up? Remember, the term 'flip' is defined as reversing the current position of the coin. Answer the Yes or No question. GPT-4 (Correct): Yes.</p><p>Question: (original) A coin is heads up. kinta does not flip the coin. renit does not flip the coin. Is the coin still heads up? Flip means reverse. (rephrased) The original question gives us a scenario wherein a coin is initially placed in a heads up position. It also states that two individuals, Kinta and Renit, have not interacted or manipulated the coin, specifically, they have not flipped it. To flip, as described, is to reverse the coin's present state. Considering these points, is the coin's position unchanged and is it still facing heads up? Answer the Yes or No question. GPT-4 (Correct): Yes.</p><p>Question: (original) A coin is heads up. brandarius does not flip the coin. gitta flips the coin. Is the coin still heads up? Flip means reverse. (rephrased) The current state of a coin is that it is facing heads up. An individual named brandarius does not alter this state by flipping the coin. However, another individual named gitta does change the state of the coin by flipping it. A flip in this context refers to reversing the side of the coin that is facing up. Hence, is it possible that following gitta's action on the coin, it remains heads up? Answer the Yes or No question. GPT-4 (Correct): No.</p><p>Table <ref type="table" target="#tab_5">17</ref>: Examples of correct and incorrect responses by GPT-4 to the original questions of Last letter concatenation (2). Question: Take the last letters of the words in "Beth Joel" and concatenate them. GPT-4 (Incorrect): "hel"</p><p>Question: Take the last letters of the words in "Patrick Lorena" and concatenate them.</p>
<head>GPT-4 (Incorrect): kkna</head><p>Question: Take the last letters of the words in "Edgar Bob" and concatenate them. GPT-4 (Correct): "rb"</p><p>Question: Take the last letters of the words in "Alexandra Betty" and concatenate them.</p>
<head>GPT-4 (Incorrect): ara</head><p>Question: Take the last letters of the words in "Paul Liz" and concatenate them.</p>
<head>GPT-4 (Incorrect): uliz</head><p>Question: Take the last letters of the words in "Sheila Alberto" and concatenate them. GPT-4 (Correct): "ao" </p><figure xml:id="fig_0"><head /><label /><figDesc>last letters of the words in 'Edgar Bob' and concatenate them." Rephrase and expand the question, and respond Could you please form a new string or series of characters by joining together the final letters from each word in the phrase "Edgar Bob"?</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Demonstration of RaR: one single prompt to improve LLM's response quality.</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Accuracy (%) comparison of different prompts using GPT-4. RaR improves the accuracy of GPT-4 across 10 tasks. A larger margin indicates a less well-designed question. For tasks such as CSQA where the question quality is good enough, our method still offers a marginal improvement.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Few-shot CoT can effectively instruct the LLM to reason in a similar manner to the provided examples. A badly crafted QA example, as shown in red, results in the LLM following the provided logic but reaching an arbitrary answer.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy (%) of GPT-4-0613, GPT-3.5-turbo-0613 and Vicuna-13b when testing on original and self-rephrased questions using Two-step RaR. GPT-4 enjoys larger improvement on various tasks compared with GPT -3.5 and Vicuna. B.1 Rephrasings from Different LLMs</figDesc></figure>
<figure type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of GPT-4's rephrased questions with Vicuna's self-rephrased questions. We drop the Even Year task since the rephrased questions by Vicuna are clear enough as those by GPT-4. Discussion with Zero-shot CoT It is widely known that zero-shot CoT, by appending the instruction "Let's think step by step." to queries, can effectively improve the performance of LLMs on reasoning tasks. However, we highlight some examples where zero-shot CoT fails to deliver improvements, sometimes even leading to diminished performance. In contrast, RaR consistently demonstrates effectiveness. Lastly, we note that our method is complementary to zero-shot CoT and can be combined together by simply adding "let's think step by step" to (3.1).</figDesc><table><row><cell /><cell cols="6">Even day Even month CSQA Sports Dates Last letter (2)</cell></row><row><cell>Vicuna-rephrased</cell><cell>58.10</cell><cell>56.19</cell><cell>51.36</cell><cell>65.00</cell><cell>32.79</cell><cell>5.45</cell></row><row><cell>GPT-4-rephrased</cell><cell>61.90</cell><cell>60.95</cell><cell>55.00</cell><cell>73.64</cell><cell>37.67</cell><cell>10.45</cell></row><row><cell cols="3">6 Discussion with Chain-of-Thought</cell><cell /><cell /><cell /><cell /></row><row><cell>6.1</cell><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of GPT-4 using original questions, zero-shot CoT prompt and RaR. Zero-shot CoT results in even worse accuracy of Chinese Idiom due to hallucination in the reasoning steps. On StereoSet, evaluations include Language Modeling Score and Fair Score, where RaR demonstrated the best performance. Preprint using the supplied human logic. Nonetheless, a concern emerges: How do LLMs respond when the human-crafted examples are flawed or contain errors? As corroborated by a recent parallel study<ref type="bibr" target="#b21">(Pawelczyk et al., 2023)</ref>, we similarly observe that LLMs can be adversely influenced by bad few-shot examples.</figDesc><table><row><cell>Dataset</cell><cell>Evaluation</cell><cell cols="3">Original Zero-shot CoT RaR</cell></row><row><cell>Chinese Idiom</cell><cell>Accuracy (%)</cell><cell>32.38</cell><cell>31.43</cell><cell>35.24</cell></row><row><cell>StereoSet</cell><cell>Language Modeling Score (%) Fair Score (%)</cell><cell>84.09 6.82</cell><cell>84.09 35.00</cell><cell>97.73 42.27</cell></row></table><note><p><p><p><p>6.2 Improvement on Few-Shot CoT</p>Few-shot CoT</p><ref type="bibr" target="#b39">(Wei et al., 2022)</ref></p>, while different from the zero-shot setting that we consider, has been the most effective CoT technique for its usage of few-shot examples. Instead of aligning the question to what the LLM best receives, few-shot CoT guides the LLM to reason</p></note></figure>
<figure type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc /><table><row><cell /><cell cols="2">Last letter (2) Last letter (4)</cell></row><row><cell>One-shot CoT</cell><cell>89.04</cell><cell>78.18</cell></row><row><cell>RaR+One-shot CoT</cell><cell>100</cell><cell>93.64</cell></row><row><cell>Four-shot CoT</cell><cell>87.21</cell><cell>52.27</cell></row><row><cell>RaR+Four-shot CoT</cell><cell>100</cell><cell>95.45</cell></row></table><note><p><p><p>Accuracy (%) of GPT-4 on Last Letter Concatenation task. We use one-shot and four-shot examples from</p><ref type="bibr" target="#b39">(Wei et al., 2022)</ref></p>, only changing the intermediate steps to be "first letter" as shown in our example.</p></note></figure>
<figure type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Few-shot examples improved by RaR used in Sec 6.2 for Last Letter Concatenation.</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Few-shot examples improved by RaR used in Sec 6.2 for Last Letter Concatenation.</figDesc><table /></figure>
<figure type="table" xml:id="tab_7"><head>-0613 Vicuna-13b-v1.5 GPT-3.5-turbo-0613</head><label /><figDesc /><table><row><cell cols="4">.04 50.45 99.54 56.19 26.36 94.55 64.15 50.48 74.09 86.36 98.64 86.36 GPT-451.43 70.73 78.64 84.55 74.80 81.82 53.33 48.57 63.81 51.43 75.24 61.36 99.04 88.57 75.24 63.81 78.18 72.73 73.64 67.27 52.57 52.03 22.38 35.24 59.36 6.82 57,73 78.18 56.19 84.55 71.42 48.57 51.43 72.73 64.15 53.33 50.48 26.36 78.64 51.43 73.64 67.27 71.42 81.82 52.03 6.82 74.09 52.05 61.36 67.27 67.62 67.72 67.72 67.62 52.57 70.73 67.27 59.36 74.80 98.64 57,73</cell><cell>75.96 66.35 19.51 48.57 52.38 49.52 58.10 51.43 56.19 60.45 72.27 32.79 65.00 55.91 51.36 2.73 5.45 69.09</cell></row><row><cell>98.18 98.18</cell><cell>88.64 88.64</cell><cell>82.65</cell><cell>82.65</cell></row></table></figure>
<figure type="table" xml:id="tab_8"><head /><label /><figDesc>Rephrase times Self-rephrased Question 1stDid the birth of Abraham Lincoln, the sixteenth president of the United States, occur on a day of the month that is considered an even number?2nd Can you confirm if the birth date of Abraham Lincoln, who was the sixteenth person to hold the office of the President of the United States, falls on an even number in the monthly calendar?3rdCould you verify whether the day of the month on which Abraham Lincoln, the sixteenth president of the United States, was born, is an even number? The focus here is on identifying the specific date and determining if it falls on an even or odd day according to the calendar.1stIs the date of Abraham Lincoln's birth even day? Provide more details, if possible, about his birth date.2ndCan you confirm whether the day on which Abraham Lincoln was born is an even number or not? Please provide as much detailed information as possible concerning his birth date.3rdCould you verify if the day of the month on which Abraham Lincoln was born falls on an even number, please provide a comprehensive description around his birth date, while emphasizing on the day of his birth especially, to assist with this confirmation?</figDesc><table /></figure>
<figure type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Examples of correct and incorrect responses by GPT-4 to the original questions of</figDesc><table><row><cell>Even day.</cell></row><row><cell>Question: Was Barack Obama born in an even day?</cell></row><row><cell>GPT-4 (Incorrect): No, Barack Obama was born on August 4, 1961, which is an even date</cell></row><row><cell>but an odd day.</cell></row><row><cell>Question: Was Kanye West born in an even day?</cell></row><row><cell>GPT-4 (Incorrect): No, Kanye West was born on June 8, 1977, which is an even day.</cell></row></table></figure>
<figure type="table" xml:id="tab_10"><head>Table 15 :</head><label>15</label><figDesc>Examples of correct and incorrect responses by GPT-4 using 1-step RaR on Coin flip. Question: "A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin still heads up? Flip means reverse." Rephrase and expand the question, and respond. Answer the Yes or No question.GPT-4 (Correct): The coin started out being heads up. Then, Aluino flipped the coin, reversing its side. After that, Arthor also flipped the coin, reversing its side again. Is the coin facing heads up now? Considering the action of flipping the coin means to reverse its side, after two flips, the coin would indeed be back in its original position. So, yes, the coin is still heads up. "A coin is heads up. brandarius does not flip the coin. gitta flips the coin. Is the coin still heads up? Flip means reverse." Rephrase and expand the question, and respond. Answer the Yes or No question. GPT-4 (Correct): Rephrased and expanded question: Initially, the coin was facing heads up. The scenario suggests that brandarius doesn't flip or reverse the coin but gitta does.</figDesc><table /><note><p><p><p>Question: "A coin is heads up. jumaimah flips the coin. buity flips the coin. Is the coin still heads up? Flip means reverse." Rephrase and expand the question, and respond. Answer the Yes or No question. GPT-4 (Correct): The coin is initially facing heads up. Then, it is flipped once by Jumaimah and again by Buity. After these two flips, is the coin still facing heads up? Taking into consideration that flipping the coin means to turn it over to its opposite side, the answer is Yes, the coin is still heads up after two flips.</p>Question: "A coin is heads up. kinta does not flip the coin. renit does not flip the coin. Is the coin still heads up? Flip means reverse." Rephrase and expand the question, and respond. Answer the Yes or No question. GPT-4 (Correct): Has the position of the coin, which was initially heads up, changed considering that neither Kinta nor Renit have flipped, implying reversed, the coin? The answer is yes, the coin is still heads up.</p>Question:</p></note></figure>
<figure type="table" xml:id="tab_11"><head>Table 16 :</head><label>16</label><figDesc>Examples of correct and incorrect responses by GPT-4 using 2-step RaR on Coin flip.Question: (original) A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin still heads up? Flip means reverse. (rephrased)</figDesc><table /></figure>
<figure type="table" xml:id="tab_12"><head>Table 22 :</head><label>22</label><figDesc>Examples of correct and incorrect responses by GPT-4 using Two-step RaR on Sports.Question: (original) Is the following sentence plausible? "Javier Zanetti earned a direct kick" (rephrased) Is it believable or possible that Javier Zanetti secured a direct free kick in a match? Use your answer for the rephrased question to answer the original question. Answer the Yes or No question. GPT-4 (Correct): Yes Question: (original) Is the following sentence plausible? "Jayson Tatum nutmegged the defender" (rephrased) Is it believable that Jayson Tatum, a professional basketball player, could have performed a move where he passed the ball through a defender's legs, also known as a 'nutmeg'? Use your answer for the question to answer the original question. Answer the Yes or No question. (original) Is the following sentence plausible? "Sean Couturier passed the puck in the Stanley Cup" (rephrased) Could it be possible that Sean Couturier passed the puck during a game in the Stanley Cup tournament? Use your answer for the rephrased question to answer the original question. Answer the Yes or No question. (original) Is the following sentence plausible? "Tuukka Rask hit a double" (rephrased) Is it feasible that Tuukka Rask, presumably known for a non-baseball sport, hit a double in a baseball game? Use your answer for the rephrased question to answer the original question. Answer the Yes or No question. (original) Is the following sentence plausible? "Sam Darnold struck out" (rephrased) Is it believable that Sam Darnold has struck out in a game? Please provide a detailed explanation supporting your answer without omitting any information from the initial question. Use your answer for the rephrased question to answer the original question. Answer the Yes or No question.</figDesc><table /><note><p>GPT-4 (Incorrect): Yes Question: GPT-4 (Correct): Yes Question: GPT-4 (Correct): No Question: GPT-4 (Correct): No</p></note></figure>
                        <note place="foot" n="1" xml:id="foot_0"><p>As the data are not open-sourced, we let GPT-4 generate 105 famous individuals with their birth dates and 105 Chinese idioms in its knowledge.</p></note>
                        <note place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/skrishna/coin_flip</p></note>
                        <note place="foot" n="3" xml:id="foot_2"><p>We note that all our experiments accessed GPT-4 during 10/01-10/30. We also include results on GPT-4-0613.</p></note>
                </body>
'''